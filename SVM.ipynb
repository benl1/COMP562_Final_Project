{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(path):\n",
    "    # Put files into lists and return them as one list of size 4\n",
    "    image_files = sorted([os.path.join(path, file)\n",
    "         for file in os.listdir(path)])\n",
    "    return image_files\n",
    "\n",
    "# Display one image\n",
    "def display_one(a, title1=\"Original\"):\n",
    "    plt.imshow(a), plt.title(title1)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Display two images\n",
    "def display(a, b, title1=\"Original\", title2=\"Edited\"):\n",
    "    plt.subplot(121), plt.imshow(a), plt.title(title1)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(122), plt.imshow(b), plt.title(title2)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "def processing(data):\n",
    "    # loading image\n",
    "    # Getting 3 images to work with\n",
    "    img = [cv2.imread('./posters/' + i, cv2.IMREAD_UNCHANGED) for i in data[:len(data)]]\n",
    "    # --------------------------------\n",
    "    # setting dim of the resize\n",
    "    height = 220\n",
    "    width = 220\n",
    "    dim = (width, height)\n",
    "    res_img = []\n",
    "    original = []\n",
    "    no_noise = []\n",
    "    results = {}\n",
    "    for i in range(len(img)):\n",
    "        #print(i)\n",
    "        res = cv2.resize(img[i], dim, interpolation=cv2.INTER_LINEAR)\n",
    "        res_img.append(res)\n",
    "\n",
    "    # Visualizing one of the images in the array\n",
    "        original.append(res_img[i])\n",
    "        if i == 0:\n",
    "            display_one(original[i])\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Remove noise\n",
    "    # Gaussian\n",
    "    #for i in range(len(res_img)):\n",
    "        blur = cv2.GaussianBlur(res_img[i], (5, 5), 0)\n",
    "        no_noise.append(blur)\n",
    "\n",
    "\n",
    "        image = no_noise[i]\n",
    "        if i == 0:\n",
    "            display(original[i], image, 'Original', 'Blurred')\n",
    "    #---------------------------------\n",
    "\n",
    "        # Segmentation\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "        # Displaying segmented images\n",
    "        if i == 0:\n",
    "            display(original[i], thresh, 'Original', 'Segmented')\n",
    "        results[i] = thresh\n",
    "    return(results)\n",
    "def main():\n",
    "    # calling global variable\n",
    "    global image_path\n",
    "\n",
    "    # import the data set that include the attributes for each picture with 0-1 representation, setting 'image_id' as index\n",
    "    df_attr = pd.read_csv('./MovieGenre.csv', encoding = 'latin-1')\n",
    "    df_attr.set_index('imdbId', inplace=True)\n",
    "    df_attr.replace(to_replace=-1, value=0, inplace=True)  # replace -1 by 0\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv('./MovieGenre.csv', encoding = 'latin-1')\n",
    "    saved_column = df.Genre\n",
    "    drama = []\n",
    "    partition = []\n",
    "    for x in saved_column:\n",
    "        if 'Horror' in str(x):\n",
    "            drama.append(1)\n",
    "        else:\n",
    "            drama.append(0)\n",
    "        partition = -1\n",
    "    df_attr['Horror'] = drama\n",
    "    df_attr['Partition'] = partition\n",
    "    df_par_attr = df_attr\n",
    "\n",
    "    random.seed(101)  ##so we hopefully get some data in our partitions\n",
    "\n",
    "    # number of observations we want in each group\n",
    "    NUM_OF_TRAIN = 1000\n",
    "    NUM_OF_TEST = 500\n",
    "\n",
    "    # just is just making the default -1 for everything, so if an observation doesnt get picked, it wont get used later\n",
    "    df_par_attr.loc[:, 'partition'] = -1\n",
    "\n",
    "    # 0 -> TRAINING\n",
    "    # 1 -> TEST\n",
    "\n",
    "    # setting training data\n",
    "    train_pos_ids = df_par_attr[(df_par_attr['Horror'] == 1) & (df_par_attr['partition'] == -1)].sample(\n",
    "        NUM_OF_TRAIN // 2).index\n",
    "    df_par_attr.loc[train_pos_ids, 'partition'] = 0\n",
    "    train_neg_ids = df_par_attr[(df_par_attr['Horror'] == 0) & (df_par_attr['partition'] == -1)].sample(\n",
    "        NUM_OF_TRAIN // 2).index\n",
    "    df_par_attr.loc[train_neg_ids, 'partition'] = 0\n",
    "\n",
    "    # setting test data\n",
    "    test_pos_ids = df_par_attr[(df_par_attr['Horror'] == 1) & (df_par_attr['partition'] == -1)].sample(\n",
    "        NUM_OF_TEST // 2).index\n",
    "    df_par_attr.loc[test_pos_ids, 'partition'] = 1\n",
    "    test_neg_ids = df_par_attr[(df_par_attr['Horror'] == 0) & (df_par_attr['partition'] == -1)].sample(\n",
    "        NUM_OF_TEST // 2).index\n",
    "    df_par_attr.loc[test_neg_ids, 'partition'] = 1\n",
    "\n",
    "    # display counter by partition\n",
    "    df_par_attr['partition'].value_counts().sort_index()\n",
    "    train_ids = df_par_attr['partition'].values == 0\n",
    "    test_ids = df_par_attr['partition'].values == 1\n",
    "\n",
    "    '''The var Dataset is a list with all images in the folder '''\n",
    "    dataset = loadImages(image_path)\n",
    "\n",
    "    idxs = np.where(train_ids)[0]\n",
    "    trnx = [dataset[i] for i in idxs]\n",
    "    #print(\"List of files in the folder:\\n\", trnx[:len(trnx)])\n",
    "    #print(\"--------------------------------\")\n",
    "\n",
    "    # sending all the images to pre-processing\n",
    "    pro_trnx = processing(trnx)\n",
    "\n",
    "    y = df_attr[\"Horror\"]\n",
    "    trny = [y[i] for i in idxs]\n",
    "    pro_trnx = np.array(list(pro_trnx.values()))\n",
    "    nsamples, nx, ny = pro_trnx.shape\n",
    "    pro_trnx = pro_trnx.reshape((nsamples, nx * ny))\n",
    "\n",
    "    logis = LogisticRegression().fit(pro_trnx,trny)\n",
    "\n",
    "    idxs = np.where(test_ids)[0]\n",
    "    tstx = [dataset[i] for i in idxs]\n",
    "    tsty = [y[i] for i in idxs]\n",
    "    pro_tstx = processing(tstx)\n",
    "    pro_tstx = np.array(list(pro_tstx.values()))\n",
    "    nsamples, nx, ny = pro_tstx.shape\n",
    "    pro_tstx = pro_tstx.reshape((nsamples, nx * ny))\n",
    "\n",
    "    predy = logis.predict(pro_tstx)\n",
    "    \n",
    "\n",
    "    print(\"logistic\")\n",
    "    tn, fp, fn, tp = confusion_matrix(tsty,predy).ravel()\n",
    "    print(\"True Negative Rate\", tn/250)\n",
    "    print(\"False Positive Rate\", fp/250)\n",
    "    print(\"False Negative Rate\", fn/250)\n",
    "    print(\"True Positive Rate\", tp/250)\n",
    "    print(\"Specificity\", 1-fp/250)\n",
    "    print(\"Sensitivity\", tp/250)\n",
    "    print(\"Accuracy\", sum(predy == tsty) / len(tsty))\n",
    "\n",
    "    \n",
    "    clf2 = svm.LinearSVC()\n",
    "    print(clf2.fit(pro_trnx,trny))\n",
    "    predysvm2 = clf2.predict(pro_tstx)\n",
    "    tn, fp, fn, tp = confusion_matrix(tsty,predysvm2).ravel()\n",
    "    print(\"True Negative Rate\", tn/250)\n",
    "    print(\"False Positive Rate\", fp/250)\n",
    "    print(\"False Negative Rate\", fn/250)\n",
    "    print(\"True Positive Rate\", tp/250)\n",
    "    print(\"Specificity\", 1-fp/250)\n",
    "    print(\"Sensitivity\", tp/250)\n",
    "    print(\"Accuracy\", sum(predysvm2 == tsty) / len(tsty))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "    print(\"AUROC\", metrics.auc(fpr, tpr))\n",
    "    \n",
    "    D = [i for i in np.arange(2, 5, 1)]\n",
    "    \n",
    "    for d in D:\n",
    "        clf3 = svm.LinearSVC()\n",
    "        print(clf3.fit(pro_trnx,trny))\n",
    "        predysvm3 = clf3.predict(pro_tstx)\n",
    "        tn, fp, fn, tp = confusion_matrix(tsty,predysvm3).ravel()\n",
    "        print(\"True Negative Rate\", tn/250)\n",
    "        print(\"False Positive Rate\", fp/250)\n",
    "        print(\"False Negative Rate\", fn/250)\n",
    "        print(\"True Positive Rate\", tp/250)\n",
    "        print(\"Specificity\", 1-fp/250)\n",
    "        print(\"Sensitivity\", tp/250)\n",
    "        print(\"Accuracy\", sum(predysvm3 == tsty) / len(tsty))\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "        print(\"AUROC\", metrics.auc(fpr, tpr))\n",
    "    print(\"done\")\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
